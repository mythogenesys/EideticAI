% ==============================================================================
%
%   Eidetic AI: Thesis Proposal
%   Version: Final, Uncompromising
%   Author: Your Name
%   Institution: Massachusetts Institute of Technology
%
% ==============================================================================
\documentclass[12pt,oneside]{report}

% --- Core Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{graphicx}
\usepackage[style=numeric-comp, backend=biber, sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{array}
\usepackage{microtype} % For better typography

% --- Document Geometry ---
\geometry{
  paper=a4paper,
  inner=1.2in,
  outer=1.2in,
  top=1in,
  bottom=1in,
}

% --- Typography & Colors ---
\usepackage{mathpazo} % Palatino font for text and math
\definecolor{MITRed}{RGB}{163, 31, 52}
\definecolor{MITGray}{RGB}{138, 139, 140}
\definecolor{LinkBlue}{RGB}{0, 83, 155}

% --- Hyperlinks Setup ---
\hypersetup{
  colorlinks=true,
  linkcolor=LinkBlue,
  citecolor=MITRed,
  urlcolor=LinkBlue,
  pdftitle={Eidetic AI: A Framework for Lifelong Machine Pedagogy},
  pdfauthor={Your Name},
  pdfsubject={PhD Thesis Proposal},
  pdfkeywords={Artificial Intelligence, Machine Learning, Educational Technology}
}

% --- Spacing ---
\onehalfspacing
\setlength{\parskip}{0.5\baselineskip} % Add space between paragraphs

% --- Custom Commands ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\framework}[1]{\textsc{#1}}
\newcommand{\qce}{\framework{qce}}
\newcommand{\ascendantk}{\framework{ascendant-k}}
\newcommand{\eideticai}{\textbf{Eidetic AI}}

% --- Theorem Environments ---
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]

% --- TikZ Libraries ---
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, chains, shadows}

% --- Bibliography ---
\addbibresource{references.bib}

% ==============================================================================
%                                 TITLE PAGE
% ==============================================================================
\title{
  \vspace{-1.5cm}
  \begin{flushright}
  \normalsize{PhD Thesis Proposal}
  \end{flushright}
  \vspace{1.5cm}
  \hrule
  \vspace{0.4cm}
  \textbf{\huge \eideticai:}\\[0.2cm]
  \Large A Framework for Lifelong Machine Pedagogy via\\[0.1cm] Quantum-Inspired Conceptual Inference and\\[0.1cm] Co-Evolutionary Curriculum Generation
  \vspace{0.4cm}
  \hrule
}
\author{Your Name}
\date{August 8, 2025\\[1cm]
  \textit{Submitted to the Department of Electrical Engineering and Computer Science}\\
  \textit{Massachusetts Institute of Technology}
}

\begin{document}

\begin{titlepage}
  \maketitle
  \thispagestyle{empty}
\end{titlepage}

\begin{abstract}
\noindent We introduce \textbf{Eidetic AI}, a new architectural paradigm for artificial tutors that fundamentally transcends the limitations of autoregressive models. We posit that true machine pedagogy requires not just knowledge retrieval, but conceptual synthesis. Our framework achieves this through three core innovations. First, we formalize the \emph{Quantum Conceptual Engine} (\qce), an inference mechanism that generates explanations by solving for the ground state of a dynamically constructed conceptual Hamiltonian, ensuring maximal coherence. Second, we propose \emph{ASCENDANT-K}, a multi-agent reinforcement learning (MARL) framework where teacher and critic agents co-evolve toward a Nash Equilibrium, producing curricula of unparalleled robustness. This process is fueled by a novel pipeline that distills confusion patterns from large-scale data into compact, adversarial student models. Third, we architect a \emph{Hierarchical Associative Memory Graph} with Hebbian update rules, enabling lifelong learning and adaptation over timescales of years without catastrophic forgetting. We will validate this framework through a rigorous suite of automated simulations and a capstone human randomized controlled trial, demonstrating superior pedagogical utility, transfer learning, and computational efficiency against state-of-the-art baselines.
\end{abstract}

\clearpage
\tableofcontents
\clearpage
\listoffigures
\listoftables
\clearpage

% ==============================================================================
%                                 CHAPTERS
% ==============================================================================

\chapter{Introduction}
The dominant paradigm in artificial intelligence, particularly in large language models (LLMs), is autoregressive prediction \cite{vaswani2017attention}. While extraordinarily powerful for tasks involving sequence completion and pattern matching, this approach is fundamentally one of mimicry, not synthesis. In the domain of education, this results in AI tutors that are sophisticated information retrievers \cite{lewis2020retrieval} and paraphrasers, yet lack the genuine pedagogical insight required to invent a truly novel analogy or re-frame a concept from first principles.

This thesis introduces \eideticai{}, a framework designed to move beyond mimicry towards true machine pedagogy. Our approach is founded on the premise that a pedagogical explanation is not a sequence of high-probability tokens, but rather a coherent, self-consistent constellation of concepts. We formalize this notion by recasting explanation generation as a problem in quantum-inspired optimization. The core of our system, the \emph{Quantum Conceptual Engine} (\qce{}), formulates an explanation as the ground state of a conceptual Hamiltonian---an energy function whose minimization corresponds to maximizing conceptual coherence, inspired by Hopfield networks \cite{hopfield1982neural} and Ising models \cite{lucas2014ising}.

This inference mechanism is situated within a broader ecosystem designed for robust and lifelong learning. The \ascendantk{} framework leverages co-evolutionary dynamics from multi-agent reinforcement learning \cite{sutton2018reinforcement} to harden curricula against student misconceptions, akin to a pedagogical Generative Adversarial Network \cite{goodfellow2014generative}. Finally, a hierarchical associative memory graph with Hebbian update rules provides a substrate for long-term adaptation and personalization. This document lays out the full theoretical foundation, algorithmic approach, and evaluation plan.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=1cm and 1.5cm,
        block/.style={rectangle, draw, thick, rounded corners, fill=MITGray!10, text width=4cm, align=center, minimum height=1.5cm, drop shadow},
        arrow/.style={-Latex, thick, color=MITRed}
    ]
    \node[block] (qce) {\textbf{Quantum Conceptual Engine (\qce{})}\\ \small Conceptual Inference via Hamiltonian Ground States};
    \node[block, below left=of qce] (ascendant) {\textbf{ASCENDANT-K}\\ \small Co-Evolutionary Curriculum Generation};
    \node[block, below right=of qce] (memory) {\textbf{Hierarchical Memory}\\ \small Lifelong Learning via Hebbian Graph};
    
    \draw[arrow] (ascendant.north) -- (qce.west) node[midway, left] {\small Curriculum};
    \draw[arrow] (memory.north) -- (qce.east) node[midway, right] {\small Knowledge};
    \draw[arrow, bend right=30] (qce.south) to node[midway, below left] {\small Feedback} (ascendant.east);
    \draw[arrow, bend left=30] (qce.south) to node[midway, below right] {\small Potentiate} (memory.west);
    \end{tikzpicture}
    \caption{The core architectural pillars of the \eideticai{} framework.}
    \label{fig:architecture}
\end{figure}

\chapter{Core Intellectual Contributions}
This thesis will establish and formally validate the following intellectual contributions:

\begin{enumerate}[leftmargin=*, itemsep=2pt]
  \item \textbf{A Formalism for Pedagogical Coherence.} Define and operationalize pedagogical coherence as the ground state energy of a higher-order \emph{conceptual Hamiltonian}, providing a novel, optimizable objective distinct from token-level log-likelihood.
  \item \textbf{An Approximation Guarantee for QCE.} Prove that for a chosen class of sparse Hamiltonians, a continuous relaxation plus projected gradient solver achieves a provable $\varepsilon$-approximation guarantee.
  \item \textbf{Convergence Guarantees for Co-evolutionary Curriculum.} Characterize \ascendantk{} as a general-sum game and prove conditions under which a population-based training algorithm converges to a correlated equilibrium.
  \item \textbf{A High-Performance QCE Implementation.} Deliver an open-source C++/CUDA \qce{}-Core with measurable performance targets (latency and memory budgets).
  \item \textbf{Hierarchical Associative Memory for Lifelong Pedagogy.} Design a multi-layer memory graph with Hebbian potentiation that supports long-term personalization and prevents catastrophic forgetting.
  \item \textbf{End-to-End Empirical Validation.} Demonstrate improved pedagogical outcomes in simulations and a human RCT against GPT-class LLMs with RAG and standard intelligent tutoring systems.
\end{enumerate}

\chapter{Formal Problem Statement}
We state three formal problems corresponding to the three thrusts of the thesis.

\begin{definition}[Quantum-Inspired Conceptual Inference]
Given a student's latent knowledge state $s \in \mathcal{S}$ and a universe of concepts $\mathcal{C}=\{c_1,\ldots,c_N\}$, construct a higher-order energy function (the \emph{conceptual Hamiltonian}) $H(\bm{x}; s, \mathcal{C})$, where $\bm{x} \in \{0,1\}^{N}$ is a binary indicator vector representing an explanation. The inference task is to find the ground state:
\[
\bm{x}^\ast = \argmin_{\bm{x}\in\{0,1\}^N} H(\bm{x}; s, \mathcal{C}).
\]
\end{definition}

\begin{hypothesis}[QCE Superiority]
For a fixed computational budget, explanations derived from the QCE ground state will yield a $>20\%$ increase in measured conceptual coherence and a statistically significant improvement in simulated student learning gain (Normalized Learning Gain; Cohen's $d \ge 0.3$, $p<0.05$) compared to explanations from state-of-the-art autoregressive LLMs with RAG.
\end{hypothesis}

\begin{definition}[Co-Evolutionary Curriculum Generation]
Let the teacher be an agent $T_\theta$ and let the critic (distilled student/adversary) be $S_\psi$. Define a two-player general-sum game with payoffs derived from student improvement and fragility. The aim is to find an equilibrium $(\theta^\ast,\psi^\ast)$ that yields curricula robust to adversarially distilled student failure modes.
\end{definition}

\begin{hypothesis}[\ascendantk{} Efficiency]
Curricula generated by \ascendantk{} will enable simulated student agents to achieve concept mastery with a $>25\%$ reduction in Time-to-Mastery (TTM) and a $>15\%$ increase in long-term retention compared to expert-designed syllabi and SOTA adaptive tutoring baselines.
\end{hypothesis}

\begin{definition}[Lifelong Pedagogical Memory]
Design a memory architecture $\mathcal{M}$ as a hierarchical associative graph that stores and retrieves personalized pedagogical strategies and supports updates via biologically-inspired potentiation rules while avoiding catastrophic forgetting.
\end{definition}

\begin{hypothesis}[Memory Graph Robustness]
A hierarchical associative memory graph with Hebbian updates will maintain $>95\%$ efficacy on previously mastered personalized teaching strategies after one simulated year of interaction and demonstrate $>50\%$ improvement in zero-shot transfer performance compared to standard RAG pipelines.
\end{hypothesis}

\chapter{Mathematical Framework}
\section{The Quantum Conceptual Engine (\qce{})}
We model an explanation as a binary vector $\bm{x}\in\{0,1\}^N$. The conceptual Hamiltonian is an order-$p$ polynomial:
\begin{equation}\label{eq:Hamiltonian}
H(\bm{x};s) \;=\; \sum_{i} a_i(s)\, x_i \;+\; \sum_{i<j} b_{ij}(s)\, x_i x_j \;+\; \sum_{i<j<k} T_{ijk}(s)\, x_i x_j x_k \;+\; \cdots
\end{equation}
The coefficients are outputs of a learned \emph{Hamiltonian Constructor} network $M_\Phi(s,\mathcal{C})$. Inference is the combinatorial optimization:
\[
\bm{x}^\ast=\argmin_{\bm{x}\in\{0,1\}^N} H(\bm{x};s),
\]
which is QUBO-like and generally NP-hard \cite{lucas2014ising}.

\section{\ascendantk{} MARL Framework}
The teacher $T_\theta$ has policy $\pi_T(a\mid s,c)$ producing actions $a$, and the critic $S_\psi$ has policy $\pi_S(\delta\mid s,a)$ producing failure modes $\delta$. The teacher's objective is:
\begin{equation}\label{eq:teacher_obj}
\max_\theta \; \E_{s,c\sim\mathcal{D}}\big[\mathcal{R}_{\text{learn}}(s,a) - \lambda \mathcal{R}_{\text{fragility}}(s,a,\delta)\big],
\end{equation}
where $a\sim\pi_T(\cdot\mid s,c)$ and $\delta\sim\pi_S(\cdot\mid s,a)$. Training uses population-based co-evolution.

\section{Hierarchical Associative Memory}
The memory is a directed weighted graph $\mathcal{G}=(V,E,W)$. After a successful interaction yielding utility $U>\tau$, activated node pairs $(i,j)$ receive a potentiation update:
\begin{equation}\label{eq:hebbian}
\Delta w_{ij} = \eta \cdot U \cdot \text{act}(c_i)\,\text{act}(c_j) - \gamma w_{ij},
\end{equation}
with learning rate $\eta$ and decay $\gamma>0$ ensuring boundedness.

\chapter{Theoretical Roadmap}
\begin{theorem}[QCE Solver Approximation Guarantee]\label{thm:qce-approx}
Let $H(\bm{x})$ be an order-$p$ Hamiltonian whose interaction tensors satisfy a $k$-sparsity condition and suppose the Hamiltonian exhibits a spectral gap $\Delta>0$. For the continuous relaxation $\widetilde{H}$ with a suitable convexifying regularizer, our projected gradient solver yields a solution $\hat{\bm{x}}$ satisfying
\[
H(\hat{\bm{x}}) - H(\bm{x}^\ast) \le \varepsilon(N,k,p,\Delta),
\]
where $\varepsilon(\cdot)$ is a function whose explicit form will be derived.
\end{theorem}

\begin{proposition}[\ascendantk{} Convergence]\label{prop:ascendant-conv}
Under assumptions of bounded rewards and Lipschitz continuity of the expected payoff, the empirical distribution over policies in our population-based co-evolutionary training converges to a correlated equilibrium of the underlying general-sum game.
\end{proposition}

\begin{lemma}[Memory Graph Stability]\label{lem:memory-stable}
The Hebbian update rule \eqref{eq:hebbian} with decay $\gamma>0$ and bounded utilities ensures the total weight of the memory graph remains bounded for all time, preventing unbounded potentiation.
\end{lemma}

\chapter{Methods, Systems, and Implementation Plan}
\section{Thrust 1: QCE Core Implementation \& Prototype}
\textbf{High-Performance Solvers (QCE-Core).} A dedicated C++/CUDA library implementing parallel simulated annealing and sparse-tensor iterative methods. PyBind11 bindings provide a Python interface.
\textbf{Performance Targets.} Median inference latency $\le 150$ ms for $N=256$ concepts on a single consumer-grade GPU.

\section{Thrust 2: \ascendantk{} Training Environment and Baselines}
\textbf{Student Confusion Distillation.} Process large anonymized datasets (e.g., ASSISTments) to construct compact student models via contrastive learning.
\textbf{Baselines.} GPT-4-class LLM + RAG, vector-store retrieval, and standard ITS algorithms.

\section{Thrust 3: Hierarchical Associative Memory \& Human Validation}
\textbf{Memory implementation.} Multi-layer graph in a high-performance graph database.
\textbf{Human RCT plan.} An IRB-approved, three-arm (Eidetic, GPT-4 tutor, control) randomized controlled trial ($N \approx 200$) will measure NLG and retention.

\chapter{Evaluation Framework \& Experiments}
\section{Datasets and Benchmarks}
ASSISTments, EdNet, and synthetic curricula for Newtonian physics and linear algebra.

\section{Metrics}
Normalized Learning Gain (NLG), Time-to-Mastery (TTM), Retention Index, Conceptual Novelty Score, and system performance metrics (latency, memory).

\section{Key Experiments}
\begin{enumerate}[itemsep=2pt, leftmargin=*]
  \item \textbf{Offline Head-to-Head:} QCE vs GPT-4+RAG on a 600-prompt benchmark.
  \item \textbf{Simulated Curriculum Training:} \ascendantk{} vs baselines on TTM and retention.
  \item \textbf{Human RCT:} The capstone experiment measuring real-world learning gains.
  \item \textbf{Longitudinal Simulation:} A year-long simulated run to measure catastrophic forgetting.
\end{enumerate}

\chapter{Timeline and Milestones}
\begin{table}[H]
\centering
\caption{High-level timeline and milestones.}
\label{tab:timeline}
\begin{tabular}{>{\bfseries}p{2.5cm} p{10cm}}
\toprule
\textbf{Year} & \textbf{Milestones} \\
\midrule
Year 1 & Formalize theorems; implement Python prototype; initial C++ kernel; workshop paper (NeurIPS/ICLR); IRB submission. \\
Year 2 & Full C++/CUDA QCE-Core release; \ascendantk{} implementation and large-scale simulated experiments; major conference paper (ICML/NeurIPS). \\
Year 3 & Memory Graph integration; conduct powered RCT; longitudinal simulations; journal submission (JMLR/AIED). \\
Year 4 & Final benchmarking; thesis composition; public release of all code, models, and (IRB-permitted) anonymized datasets. \\
\bottomrule
\end{tabular}
\end{table}

\chapter{Deliverables}
By the end of the PhD, the project will produce:
\begin{itemize}[itemsep=2pt, leftmargin=*]
  \item \textbf{QCE-Core Library:} An open-source C++/CUDA library.
  \item \textbf{\ascendantk{} Framework:} An open-source MARL training pipeline.
  \item \textbf{Benchmarks \& Datasets:} Reproducible benchmark harnesses and anonymized datasets.
  \item \textbf{Three Peer-reviewed publications.}
  \item \textbf{The PhD Thesis.}
\end{itemize}

\clearpage
\appendix
\chapter{References}
\printbibliography

\end{document}